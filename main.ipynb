{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Captura de mudanças no ambiente através da webcam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "# Abrir o vídeo\n",
    "cam = cv2.VideoCapture(0)  # Troque pelo caminho do seu vídeo\n",
    "\n",
    "# Ler o primeiro frame como referência de fundo\n",
    "ret, frame_reference = cam.read()\n",
    "if not ret:\n",
    "    print(\"Erro ao carregar o vídeo\")\n",
    "    cam.release()\n",
    "    exit()\n",
    "\n",
    "frame_reference = cv2.flip(frame_reference, 1) \n",
    "# Converter o frame de referência para escala de cinza\n",
    "frame_reference_gray = cv2.cvtColor(frame_reference, cv2.COLOR_BGR2GRAY)\n",
    "frame_reference_gray = cv2.GaussianBlur(frame_reference_gray, (21, 21), 0)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cam.read()\n",
    "\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    frame = cv2.flip(frame, 1) \n",
    "    # frame = cv2.resize(frame, (0,0), fx=0.5, fy=0.5)\n",
    "    # Converter o frame atual para escala de cinza e aplicar suavização\n",
    "    frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    frame_gray = cv2.GaussianBlur(frame_gray, (21, 21), 0)\n",
    "\n",
    "    # Subtrair o frame de referência do frame atual para detectar mudanças\n",
    "    frame_delta = cv2.absdiff(frame_reference_gray, frame_gray)\n",
    "\n",
    "    # Aplicar o thresholding para obter a imagem binária das mudanças\n",
    "    _, thresh = cv2.threshold(frame_delta, 25, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "    # Dilatar a imagem para preencher buracos nos contornos\n",
    "    thresh = cv2.dilate(thresh, None, iterations=2)\n",
    "\n",
    "    # Encontrar contornos das mudanças\n",
    "    contours, _ = cv2.findContours(thresh.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    # Desenhar retângulos ao redor das áreas detectadas\n",
    "    for contour in contours:\n",
    "        if cv2.contourArea(contour) < 500:  # Ignorar pequenas áreas\n",
    "            continue\n",
    "        (x, y, w, h) = cv2.boundingRect(contour)\n",
    "        cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "\n",
    "    # Mostrar o frame com as detecções\n",
    "    cv2.imshow('Deteccao de Mudancas', frame)\n",
    "    cv2.imshow('Thresh', thresh)\n",
    "    cv2.imshow('Frame Delta', frame_delta)\n",
    "\n",
    "    # Pressione 'q' para sair\n",
    "    if cv2.waitKey(30) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Liberar o vídeo e fechar as janelas\n",
    "cam.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Captura de mudanças no ambiente de um vídeo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "# Abrir o vídeo\n",
    "cap = cv2.VideoCapture('../notebooks/heavy_object.mp4')  # Troque pelo caminho do seu vídeo\n",
    "\n",
    "# Ler o primeiro frame como referência de fundo\n",
    "ret, frame_reference = cap.read()\n",
    "if not ret:\n",
    "    print(\"Erro ao carregar o vídeo\")\n",
    "    cap.release()\n",
    "    exit()\n",
    "\n",
    "frame_reference = cv2.resize(frame_reference, (0,0), fx=0.5, fy=0.5)\n",
    "\n",
    "# Converter o frame de referência para escala de cinza\n",
    "frame_reference_gray = cv2.cvtColor(frame_reference, cv2.COLOR_BGR2GRAY)\n",
    "frame_reference_gray = cv2.GaussianBlur(frame_reference_gray, (21, 21), 0)\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    frame = cv2.resize(frame, (0,0), fx=0.5, fy=0.5)\n",
    "    # Converter o frame atual para escala de cinza e aplicar suavização\n",
    "    frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    frame_gray = cv2.GaussianBlur(frame_gray, (21, 21), 0)\n",
    "\n",
    "    # Subtrair o frame de referência do frame atual para detectar mudanças\n",
    "    frame_delta = cv2.absdiff(frame_reference_gray, frame_gray)\n",
    "\n",
    "    # Aplicar o thresholding para obter a imagem binária das mudanças\n",
    "    _, thresh = cv2.threshold(frame_delta, 25, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "    # Dilatar a imagem para preencher buracos nos contornos\n",
    "    thresh = cv2.dilate(thresh, None, iterations=2)\n",
    "\n",
    "    # Encontrar contornos das mudanças\n",
    "    contours, _ = cv2.findContours(thresh.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    # Desenhar retângulos ao redor das áreas detectadas\n",
    "    for contour in contours:\n",
    "        if cv2.contourArea(contour) < 500:  # Ignorar pequenas áreas\n",
    "            continue\n",
    "        (x, y, w, h) = cv2.boundingRect(contour)\n",
    "        cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "\n",
    "    # Mostrar o frame com as detecções\n",
    "    cv2.imshow('Deteccao de Mudancas', frame)\n",
    "    cv2.imshow('Thresh', thresh)\n",
    "    cv2.imshow('Frame Delta', frame_delta)\n",
    "\n",
    "    # Pressione 'q' para sair\n",
    "    if cv2.waitKey(30) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Liberar o vídeo e fechar as janelas\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reconhecimento de expressões faciais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import dlib\n",
    "from imutils import face_utils\n",
    "\n",
    "# Carregar o detector de faces do dlib e o preditor de landmarks faciais\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor('shape_predictor_68_face_landmarks.dat')  # Caminho do arquivo baixado\n",
    "\n",
    "# Função para calcular a razão de aspecto da boca (detecção de sorriso)\n",
    "def mouth_aspect_ratio(mouth):\n",
    "    A = cv2.norm(mouth[3] - mouth[9])  # Distância vertical\n",
    "    B = cv2.norm(mouth[0] - mouth[6])  # Distância horizontal\n",
    "    mar = A / B\n",
    "    return mar\n",
    "\n",
    "# Abrir o vídeo\n",
    "cap = cv2.VideoCapture(0)  # Substitua pelo caminho do seu vídeo\n",
    "\n",
    "# Laço de leitura do vídeo\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    frame = cv2.flip(frame, 1) \n",
    "    # Converter o frame para escala de cinza\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Detectar rostos no frame\n",
    "    faces = detector(gray)\n",
    "\n",
    "    for face in faces:\n",
    "        # Prever landmarks faciais\n",
    "        shape = predictor(gray, face)\n",
    "        shape = face_utils.shape_to_np(shape)\n",
    "\n",
    "        # Pegar coordenadas da boca (pontos de 48 a 68)\n",
    "        mouth = shape[48:68]\n",
    "\n",
    "        # Desenhar a boca no frame\n",
    "        cv2.drawContours(frame, [mouth], -1, (0, 255, 0), 1)\n",
    "\n",
    "        # Calcular a razão de aspecto da boca (indicador de sorriso)\n",
    "        mar = mouth_aspect_ratio(mouth)\n",
    "\n",
    "        # Definir um limite para detecção de sorriso\n",
    "        if mar > 0.4:\n",
    "            cv2.putText(frame, \"Sorrindo\", (face.left(), face.top() - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "        else:\n",
    "            cv2.putText(frame, \"Serio\", (face.left(), face.top() - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "    # Mostrar o frame com a detecção\n",
    "    cv2.imshow('Reconhecimento de Expressoes', frame)\n",
    "\n",
    "    # Pressione 'q' para sair\n",
    "    if cv2.waitKey(30) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Liberar o vídeo e fechar as janelas\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eye tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import dlib\n",
    "import numpy as np\n",
    "from imutils import face_utils\n",
    "\n",
    "# Carregar o detector de faces do dlib e o preditor de landmarks faciais\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor('shape_predictor_68_face_landmarks.dat')  # Caminho para o arquivo baixado\n",
    "\n",
    "def get_eye_center(eye):\n",
    "    # Calcular o centro do olho com base na média das coordenadas dos landmarks\n",
    "    center_x = int(np.mean(eye[:, 0]))\n",
    "    center_y = int(np.mean(eye[:, 1]))\n",
    "    return (center_x, center_y)\n",
    "\n",
    "# Função para desenhar uma linha sobre o olho\n",
    "def draw_eye_lines(eye, frame):\n",
    "    # Desenhar as linhas ao redor do olho\n",
    "    for i in range(1, len(eye)):\n",
    "        cv2.line(frame, tuple(eye[i - 1]), tuple(eye[i]), (0, 255, 0), 1)\n",
    "    cv2.line(frame, tuple(eye[0]), tuple(eye[-1]), (0, 255, 0), 1)\n",
    "\n",
    "# Abrir o vídeo\n",
    "cap = cv2.VideoCapture(0)  # Use 0 para webcam ou substitua pelo caminho de um vídeo\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Converter o frame para escala de cinza\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Detectar rostos no frame\n",
    "    faces = detector(gray)\n",
    "\n",
    "    for face in faces:\n",
    "        # Prever landmarks faciais\n",
    "        shape = predictor(gray, face)\n",
    "        shape = face_utils.shape_to_np(shape)\n",
    "\n",
    "        # Obter as coordenadas dos olhos (landmarks 36-41 para o olho esquerdo, 42-47 para o olho direito)\n",
    "        left_eye = shape[36:42]\n",
    "        right_eye = shape[42:48]\n",
    "\n",
    "        # Desenhar os contornos dos olhos\n",
    "        draw_eye_lines(left_eye, frame)\n",
    "        draw_eye_lines(right_eye, frame)\n",
    "\n",
    "        # Calcular o centro dos olhos\n",
    "        left_eye_center = get_eye_center(left_eye)\n",
    "        right_eye_center = get_eye_center(right_eye)\n",
    "\n",
    "        # Desenhar os centros dos olhos\n",
    "        cv2.circle(frame, left_eye_center, 3, (0, 0, 255), -1)\n",
    "        cv2.circle(frame, right_eye_center, 3, (0, 0, 255), -1)\n",
    "\n",
    "        # Desenhar retângulos ao redor dos olhos\n",
    "        l_x, l_y, l_w, l_h = cv2.boundingRect(left_eye)\n",
    "        r_x, r_y, r_w, r_h = cv2.boundingRect(right_eye)\n",
    "        cv2.rectangle(frame, (l_x, l_y), (l_x + l_w, l_y + l_h), (255, 0, 0), 1)\n",
    "        cv2.rectangle(frame, (r_x, r_y), (r_x + r_w, r_y + r_h), (255, 0, 0), 1)\n",
    "\n",
    "    # Mostrar o frame com as detecções\n",
    "    cv2.imshow('Eye Tracking', frame)\n",
    "\n",
    "    # Pressione 'q' para sair\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Liberar o vídeo e fechar as janelas\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
